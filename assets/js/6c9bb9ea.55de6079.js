"use strict";(self.webpackChunkreact_native_website=self.webpackChunkreact_native_website||[]).push([[1513],{35318:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return d}});var r=n(27378);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(n),d=a,f=u["".concat(l,".").concat(d)]||u[d]||m[d]||i;return n?r.createElement(f,o(o({ref:t},p),{},{components:n})):r.createElement(f,o({ref:t},p))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,o[1]=s;for(var c=2;c<i;c++)o[c]=n[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},83517:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return u},contentTitle:function(){return d},metadata:function(){return f},toc:function(){return g},default:function(){return y}});var r=n(35318),a=Object.defineProperty,i=Object.defineProperties,o=Object.getOwnPropertyDescriptors,s=Object.getOwnPropertySymbols,l=Object.prototype.hasOwnProperty,c=Object.prototype.propertyIsEnumerable,p=(e,t,n)=>t in e?a(e,t,{enumerable:!0,configurable:!0,writable:!0,value:n}):e[t]=n,m=(e,t)=>{for(var n in t||(t={}))l.call(t,n)&&p(e,n,t[n]);if(s)for(var n of s(t))c.call(t,n)&&p(e,n,t[n]);return e};const u={id:"Experiments",title:"Experiments"},d="Experiment Tracking:",f={unversionedId:"Experiments",id:"version-3.1/Experiments",title:"Experiments",description:"Katonic Platform provides a central place to organize your ML experimentation. With the Katonic SDK you can log and display metrics, parameters, images and other ML metadata. In the Experiment Interface you can Search, group and compare runs with no extra effort. You can see and debug experiments live as they are running and query experiment metadata programmatically.",source:"@site/versioned_docs/version-3.1/Experiments.md",sourceDirName:".",slug:"/Experiments",permalink:"/Experiments",tags:[],version:"3.1",frontMatter:{id:"Experiments",title:"Experiments"},sidebar:"User Guide",previous:{title:"File Manager",permalink:"/File-Manager"},next:{title:"Model Registry",permalink:"/Model-Registry"}},g=[{value:"Running an Experiment using Katonic SDK and Katonic Platform Interface",id:"running-an-experiment-using-katonic-sdk-and-katonic-platform-interface",children:[],level:2}],h={toc:g};function y(e){var t,a=e,{components:p}=a,u=((e,t)=>{var n={};for(var r in e)l.call(e,r)&&t.indexOf(r)<0&&(n[r]=e[r]);if(null!=e&&s)for(var r of s(e))t.indexOf(r)<0&&c.call(e,r)&&(n[r]=e[r]);return n})(a,["components"]);return(0,r.kt)("wrapper",(t=m(m({},h),u),i(t,o({components:p,mdxType:"MDXLayout"}))),(0,r.kt)("h1",m({},{id:"experiment-tracking"}),"Experiment Tracking:"),(0,r.kt)("p",null,"Katonic Platform provides a central place to organize your ML experimentation. With the Katonic SDK you can log and display metrics, parameters, images and other ML metadata. In the Experiment Interface you can Search, group and compare runs with no extra effort. You can see and debug experiments live as they are running and query experiment metadata programmatically."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(29773).Z})),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Creating experiments:")," Experiments in Katonic essentially allows you to group your models and any relevant metrics. For example, you can compare models that you\u2019ve built in TensorFlow and in PyTorch and name this experiment something like pytorch_tensorflow. In the context of anomaly detection, you can create an experiment called model_prototyping and group all of the models that you want to test by running the training pipelines after setting model_prototyping as the experiment name. As you\u2019ll see shortly, grouping model training sessions by experiment can really help organize your workspace because you\u2019ll get a clear idea of the context behind trained models."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Model and metric logging:")," Katonic SDK allows you to save a model in a modularized form and log all of the metrics related to the model run. A model run can be thought of as the model training, testing, and validation pipeline.  It is possible for you to train, evaluate, and even validate your model, logging all of the metrics for each respective step in the whole process. allowing you to much more easily compare different hyperparameter setups all at once."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Comparing model metrics:")," Katonic Experiment interface also allows you to compare different models and their metrics all at once. And so, when performing validation to help tune a model\u2019s hyperparameters, you can compare all of the selected metrics together in Katonic platform using its Experiment interface. "),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(77072).Z})),(0,r.kt)("h2",m({},{id:"running-an-experiment-using-katonic-sdk-and-katonic-platform-interface"}),"Running an Experiment using Katonic SDK and Katonic Platform Interface"),(0,r.kt)("p",null,"This topic walks you through a simple example to help you get started with Experiments in Katonic platform."),(0,r.kt)("p",null,"Let's build a simple machine learning project, Diabetes prediction. The objective of the use case is to diagnostically predict whether or not a patient has diabetes based on certain diagnostic measurements included in the dataset."),(0,r.kt)("pre",null,(0,r.kt)("code",m({parentName:"pre"},{}),"## imports\nfrom katonic.ml.classification import Classifier\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\n## Load the Data set\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv')\ndf.head()\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(58302).Z})),(0,r.kt)("pre",null,(0,r.kt)("code",m({parentName:"pre"},{}),'x = df.drop(columns=[\'Outcome\'], axis=1)\ny = df[\'Outcome\']\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.20,random_state=98)\n\n\n## let\'s create experiment.\nexp_name = \'diabetes_prediction_classifier\'\n # Initiate the classifier instance\n\nclf = Classifier(X_train,X_test,y_train,y_test, exp_name)\nexp_id = clf.id\nprint("experiment name : ",clf.name)\nprint("experiment location : ",clf.location)\nprint("experiment id : ",clf.id)\nprint("experiment status : ",clf.stage)\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(92457).Z})),(0,r.kt)("p",null,"Now you can see an experiment of diabetes_prediction has been created. You can also see that experiment in the user interface."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(24314).Z})),(0,r.kt)("p",null,"Note: You can see the experiment, you will notice there are no model runs yet.\nIt is just an empty experiment; we will now start applying different models and compare them later.\nLet's train and log some models"),(0,r.kt)("pre",null,(0,r.kt)("code",m({parentName:"pre"},{}),"## Logistic regression\nclf.LogisticRegression()\n## Decision TreeClassifier\nclf.DecisionTreeClassifier(max_depth=8, criterion='gini', min_samples_split=3)\n")),(0,r.kt)("p",null,"Up to this point we have trained two models, as these logs all the necessary metrics implicitly, you don\u2019t have to worry about logging metrics explicitly."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(39186).Z})),(0,r.kt)("p",null,"Here you can see, the two models have been logged with the necessary metrics.\nYou can further investigate the model run by clicking any of their corresponding start time. Once you click that, you can see the following window,"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(78981).Z})),(0,r.kt)("p",null,"Here you can see the various details corresponding to given model run, the details of parameters used in the models, metrics tracked while training/testing model, associated artefacts and full path for the model registry where the serialized file of model is saved."),(0,r.kt)("p",null,"Let's come back to the previous window and compare both runs."),(0,r.kt)("p",null,"You can compare different model runs in the given experiment window."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(17973).Z})),(0,r.kt)("p",null,"First select the run, you want to compare, then click on the compare button.\nYou will be taken to new window, where you will find details about plots, parameters and metrics difference."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(30703).Z})),(0,r.kt)("p",null,"The given window page contains more details, so you need to scroll down to see further\ndetails available in the page, like run details, parameter details and metric details."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(78589).Z})),(0,r.kt)("p",null,"Now let's run a random forest model with hyperparameter tuning and compare all child run with parameters select by tuning algorithm."),(0,r.kt)("pre",null,(0,r.kt)("code",m({parentName:"pre"},{}),"## define parameters space for searching best parameters\nparams = {\n'n_estimators': {\n    'low': 80,\n    'high': 120,\n    'step': 10,\n    'type': 'int'\n    },\n'criterion':{\n    'values': ['gini', 'entropy'],\n    'type': 'categorical'\n    },\n'min_samples_split': {\n    'low': 2,\n    'high': 5,\n    'type': 'int'\n    },\n'min_samples_leaf':{\n    'low': 1,\n    'high': 5,\n    'type': 'int'\n    }\n}\n\n\n\n## train, tune and log Random Forest model.\nclf.RandomForestClassifier(is_tune=True,n_trials=5, params=params)\n")),(0,r.kt)("p",null,"Now come back to Experiment interface, you will see the + sign in front of one of the start time(corresponds to parent run), once expand that + option you will see all the child runs."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(80810).Z})),(0,r.kt)("p",null,"Now we will compare the child runs and see the parallel plots on how different params effecting the accuracy metrics (default)."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(19259).Z})),(0,r.kt)("p",null,"You can see, in the metrics tab accuracy is default variable, but you can choose other metrics also and visualize the effect of different params on the given metric value."),(0,r.kt)("p",null,"\u25cf So up to here, you have been shown"),(0,r.kt)("p",null,"\u25cf How to create experiments through SDK and interact with user interface."),(0,r.kt)("p",null,"\u25cf How to train and log models."),(0,r.kt)("p",null,"\u25cf How to compare runs of two different models."),(0,r.kt)("p",null,"\u25cf How to visualize and compare runs of hyperparameter tuned model."),(0,r.kt)("p",null,"\u25cf To train and log other models see documentation of Katonic SDK."),(0,r.kt)("p",null,"\u25cf To know how to register model, create new versions and transition model to different stages see our Model Registry Documentation."))}y.isMDXComponent=!0},29773:function(e,t,n){t.Z=n.p+"assets/images/Experiments1-25823c2afc85abaebd9cd878920069ef.png"},78589:function(e,t,n){t.Z=n.p+"assets/images/Experiments10-f983ea0b0d65405c257bb99c3558b06a.png"},80810:function(e,t,n){t.Z=n.p+"assets/images/Experiments11-7a4f7bad61a17c94e26eddd24b7ab393.png"},19259:function(e,t,n){t.Z=n.p+"assets/images/Experiments12-2351cf0db6ed9d77248ee214c4a18b6e.png"},77072:function(e,t,n){t.Z=n.p+"assets/images/Experiments2-0b381dfbd1dfd63a9b46cb215a5426ca.png"},58302:function(e,t,n){t.Z=n.p+"assets/images/Experiments3-2765b91a0b2dee7205a0ab896841e8fb.png"},92457:function(e,t,n){t.Z=n.p+"assets/images/Experiments4-347e4278bbe7b9a26a9cfe53b975a190.png"},24314:function(e,t,n){t.Z=n.p+"assets/images/Experiments5-7af282a3af6e9f46d82025039eceeb05.png"},39186:function(e,t,n){t.Z=n.p+"assets/images/Experiments6-82570b32443acdf96e84d4311ea9021d.png"},78981:function(e,t,n){t.Z=n.p+"assets/images/Experiments7-30744b4bd943e517dda9089325809558.png"},17973:function(e,t,n){t.Z=n.p+"assets/images/Experiments8-98aa70bfa076b1ebe4066c41c0e1e9a5.png"},30703:function(e,t,n){t.Z=n.p+"assets/images/Experiments9-d761113d4a0d37eba0955110e52b882f.png"}}]);