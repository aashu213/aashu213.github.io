"use strict";(self.webpackChunkreact_native_website=self.webpackChunkreact_native_website||[]).push([[2457],{35318:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return f}});var r=n(27378);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=r.createContext({}),c=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=c(e.components);return r.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=c(n),f=o,m=u["".concat(s,".").concat(f)]||u[f]||p[f]||a;return n?r.createElement(m,i(i({ref:t},d),{},{components:n})):r.createElement(m,i({ref:t},d))}));function f(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,i[1]=l;for(var c=2;c<a;c++)i[c]=n[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},33960:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return u},contentTitle:function(){return f},metadata:function(){return m},toc:function(){return h},default:function(){return b}});var r=n(35318),o=Object.defineProperty,a=Object.defineProperties,i=Object.getOwnPropertyDescriptors,l=Object.getOwnPropertySymbols,s=Object.prototype.hasOwnProperty,c=Object.prototype.propertyIsEnumerable,d=(e,t,n)=>t in e?o(e,t,{enumerable:!0,configurable:!0,writable:!0,value:n}):e[t]=n,p=(e,t)=>{for(var n in t||(t={}))s.call(t,n)&&d(e,n,t[n]);if(l)for(var n of l(t))c.call(t,n)&&d(e,n,t[n]);return e};const u={id:"Model-Monitoring",title:"Model Monitoring"},f=void 0,m={unversionedId:"Model-Monitoring",id:"version-3.0/Model-Monitoring",title:"Model Monitoring",description:"Model Monitoring is an operational stage in the machine learning lifecycle that comes after model deployment. It entails monitoring your ML models for changes such as model degradation, data drift, and concept drift, and ensuring that your model is maintaining an acceptable level of performance.",source:"@site/versioned_docs/version-3.0/Model-Monitoring.md",sourceDirName:".",slug:"/Model-Monitoring",permalink:"/Model-Monitoring",editUrl:"https://github.com/facebook/react-native-website/blob/master/website/../docs/Model-Monitoring.md",tags:[],version:"3.0",frontMatter:{id:"Model-Monitoring",title:"Model Monitoring"},sidebar:"User Guide",previous:{title:"Model Deployment",permalink:"/Model-Deployment"},next:{title:"App Deployment",permalink:"/App-Deployment"}},h=[{value:"For Classification Dashboard",id:"for-classification-dashboard",children:[],level:2},{value:"For Regression Dashboard",id:"for-regression-dashboard",children:[],level:2}],g={toc:h};function b(e){var t,o=e,{components:d}=o,u=((e,t)=>{var n={};for(var r in e)s.call(e,r)&&t.indexOf(r)<0&&(n[r]=e[r]);if(null!=e&&l)for(var r of l(e))t.indexOf(r)<0&&c.call(e,r)&&(n[r]=e[r]);return n})(o,["components"]);return(0,r.kt)("wrapper",(t=p(p({},g),u),a(t,i({components:d,mdxType:"MDXLayout"}))),(0,r.kt)("p",null,"Model Monitoring is an operational stage in the machine learning lifecycle that comes after model deployment. It entails monitoring your ML models for changes such as model degradation, data drift, and concept drift, and ensuring that your model is maintaining an acceptable level of performance.\nOur model monitoring tool is available to automate model evaluation and provide alerts when production models have degraded to the point where an expert should perform a detailed evaluation."),(0,r.kt)("p",null,"Use Katonic Model Monitoring for -"),(0,r.kt)("p",null,"a)\tBuilt-in Model Monitoring"),(0,r.kt)("p",null,"b)\tAutomated Retraining"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Navigate to deploy section from sidebar on the platform. "),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("img",{alt:"Untitled",src:n(85803).Z}))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Click on \u2018Monitor\u2019 to monitor the effectiveness and efficiency of your deployed model. "))),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(88009).Z})),(0,r.kt)("ol",p({},{start:3}),(0,r.kt)("li",{parentName:"ol"},"In Katonic Model Monitoring\u2019 that opens up in a new tab, you can get real-time insights and alerts on model performance and data characteristics. You can also debug anomalies and initiate trigger to execute ML production pipelines to retrain the models with new data, depending on your use case. ")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(64886).Z})),(0,r.kt)("p",null,"In the Dashboard, we provide total of 7 metrices, 3 operational and 4 model based on real time feedback, these metrices are:"),(0,r.kt)("p",null,"Operational (for classification/Regression):"),(0,r.kt)("p",null,"a.\tReal time request latency\nb.\tReal time request rate\nc.\tReal time request count"),(0,r.kt)("h2",p({},{id:"for-classification-dashboard"}),"For Classification Dashboard"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"a.\tRecall Score")),(0,r.kt)("p",null,"The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"b.\tPrecision Score")),(0,r.kt)("p",null,"The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"c.\tF1 Score")),(0,r.kt)("p",null,"The F1-score combines the precision and recall of a classifier into a single metric by taking their harmonic mean. It is primarily used to compare the performance of two classifiers. Suppose that classifier A has a higher recall, and classifier B has higher precision."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"d.\tAccuracy")),(0,r.kt)("p",null,"Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. "),(0,r.kt)("p",null," ",(0,r.kt)("img",{alt:"Untitled",src:n(81012).Z})),(0,r.kt)("h2",p({},{id:"for-regression-dashboard"}),"For Regression Dashboard"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"a.\tMean Squared Error")),(0,r.kt)("p",null,"The mean squared error (MSE) tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the \u201cerrors\u201d) and squaring them. The squaring is necessary to remove any negative signs."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"b.\tRoot Mean Squared Error")),(0,r.kt)("p",null,"Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"c.\tAbsolute Error")),(0,r.kt)("p",null,"The absolute value of the difference between an observed value of a quantity and the true value the difference between true length and measured length is called the error of measurement or absolute error."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"d.\tMean Absolute Error")),(0,r.kt)("p",null,"Mean Absolute Error is a model evaluation metric used with regression models. The mean absolute error of a model with respect to a test set is the mean of the absolute values of the individual prediction errors on over all instances in the test set."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:n(56181).Z})))}b.isMDXComponent=!0},85803:function(e,t,n){t.Z=n.p+"assets/images/ModelMonitoring1-7d4b7e6050c2d91164df1c423e55da9e.jpg"},88009:function(e,t,n){t.Z=n.p+"assets/images/ModelMonitoring2-3961f9f1e2e143da06ce8d4469b234d9.png"},64886:function(e,t,n){t.Z=n.p+"assets/images/ModelMonitoring3-7bae4d71bcfed1972a0ab34a551d2974.png"},81012:function(e,t,n){t.Z=n.p+"assets/images/ModelMonitoring4-80833f3abccdbcd67257b0ddb004b26f.png"},56181:function(e,t,n){t.Z=n.p+"assets/images/ModelMonitoring5-730bbed72c29ab6459928502ff5b554e.png"}}]);